{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from enum import Enum\n",
    "from sklearn.datasets import load_iris\n",
    "from typing import Callable, Iterable, List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperParams(Enum):\n",
    "    ACTIVATION     = tf.nn.relu\n",
    "    BATCH_SIZE     = 5\n",
    "    EPOCHS         = 500\n",
    "    HIDDEN_NEURONS = 10\n",
    "    NORMALIZER     = tf.nn.softmax\n",
    "    OUTPUT_NEURONS = 3\n",
    "    OPTIMIZER      = tf.keras.optimizers.Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "xdat = iris.data\n",
    "ydat = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tensor(self, depth: int = 3) -> None:\n",
    "        self.xtrn = tf.convert_to_tensor(self.xtrn, dtype = np.float32) \n",
    "        self.xtst = tf.convert_to_tensor(self.xtst, dtype = np.float32)\n",
    "        self.ytrn = tf.convert_to_tensor(tf.one_hot(self.ytrn, depth = depth))\n",
    "        self.ytst = tf.convert_to_tensor(tf.one_hot(self.ytst, depth = depth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data:\n",
    "    def __init__(self,xdat: np.ndarray,ydat: np.ndarray,ratio: float = 0.3) -> Tuple:\n",
    "        self.xdat = xdat\n",
    "        self.ydat = ydat\n",
    "        self.ratio = ratio\n",
    "        \n",
    "    def partition(self) -> None:\n",
    "        scnt = self.xdat.shape[0] / np.unique(self.ydat).shape[0]\n",
    "        ntst = int(self.xdat.shape[0] * self.ratio / (np.unique(self.ydat)).shape[0])\n",
    "        idx  = np.random.choice(np.arange(0, self.ydat.shape[0] / np.unique(self.ydat).shape[0], dtype = int), ntst, replace = False)\n",
    "        for i in np.arange(1, np.unique(self.ydat).shape[0]):\n",
    "            idx = np.concatenate((idx, np.random.choice(np.arange((scnt * i), scnt * (i + 1), dtype = int), ntst, replace = False)))\n",
    "\n",
    "        self.xtrn = self.xdat[np.where(~np.in1d(np.arange(0, self.ydat.shape[0]), idx))[0], :]\n",
    "        self.ytrn = self.ydat[np.where(~np.in1d(np.arange(0, self.ydat.shape[0]), idx))[0]]\n",
    "        self.xtst = self.xdat[idx, :]\n",
    "        self.ytst = self.ydat[idx]\n",
    "    \n",
    "    def to_tensor(self, depth: int = 3) -> None:\n",
    "        self.xtrn = tf.convert_to_tensor(self.xtrn, dtype=np.float32)\n",
    "        self.xtst = tf.convert_to_tensor(self.xtst, dtype=np.float32)\n",
    "        self.ytrn = tf.convert_to_tensor(tf.one_hot(self.ytrn, depth=depth))\n",
    "        self.ytst = tf.convert_to_tensor(tf.one_hot(self.ytst, depth=depth))\n",
    "        \n",
    "    def batch(self, num: int = 16) -> None:\n",
    "        try:\n",
    "            size = self.xtrn.shape[0] / num\n",
    "            if self.xtrn.shape[0] % num != 0:\n",
    "                sizes = [tf.floor(size).numpy().astype(int) for i in range(num)] + [self.xtrn.shape[0] % num]\n",
    "            else:\n",
    "                sizes = [tf.floor(size).numpy().astype(int) for i in range(num)]\n",
    "\n",
    "            self.xtrn_batches = tf.split(self.xtrn, num_or_size_splits = sizes, axis = 0)\n",
    "            self.ytrn_batches = tf.split(self.ytrn, num_or_size_splits = sizes, axis = 0)\n",
    "\n",
    "            num = int(self.xtst.shape[0] / sizes[0])\n",
    "            if self.xtst.shape[0] % sizes[0] != 0:\n",
    "                sizes = [sizes[i] for i in range(num)] + [self.xtst.shape[0] % sizes[0]]\n",
    "            else:\n",
    "                sizes = [sizes[i] for i in range(num)]\n",
    "\n",
    "            self.xtst_batches = tf.split(self.xtst, num_or_size_splits = sizes, axis = 0)\n",
    "            self.ytst_batches = tf.split(self.ytst, num_or_size_splits = sizes, axis = 0)\n",
    "        except:\n",
    "            self.xtrn_batches = [self.xtrn]\n",
    "            self.ytrn_batches = [self.ytrn]\n",
    "            self.xtst_batches = [self.xtst]\n",
    "            self.ytst_batches = [self.ytst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense:\n",
    "    def __init__(self, i: int, o: int, f: Callable[[tf.Tensor], tf.Tensor], initializer: Callable = tf.random.normal) -> None:\n",
    "        self.w = tf.Variable(initializer([i, o]))\n",
    "        self.b = tf.Variable(initializer([o]))\n",
    "        self.f = f\n",
    "\n",
    "    def __call__(self, x: tf.Tensor) -> tf.Tensor:\n",
    "        if callable(self.f):\n",
    "            return self.f(tf.add(tf.matmul(x, self.w), self.b))\n",
    "        else:\n",
    "            return tf.add(tf.matmul(x, self.w), self.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chain:\n",
    "\n",
    "    def __init__(self, layers: List[Iterable[Dense]]) -> None:\n",
    "        self.layers = layers\n",
    "    \n",
    "    def __call__(self, x: tf.Tensor) -> tf.Tensor:\n",
    "        self.out = x; self.params = []\n",
    "        for l in self.layers:\n",
    "            self.out = l(self.out)\n",
    "            self.params.append([l.w, l.b])\n",
    "        \n",
    "        self.params = [j for i in self.params for j in i]\n",
    "        return self.out\n",
    "\n",
    "    def backward(self, inputs: tf.Tensor, targets: tf.Tensor) -> None:\n",
    "        grads = self.grad(inputs, targets)\n",
    "        self.optimize(grads, 0.001)\n",
    "    \n",
    "    def loss(self, preds: tf.Tensor, targets: tf.Tensor) -> tf.Tensor:\n",
    "        return tf.reduce_mean(\n",
    "            tf.keras.losses.categorical_crossentropy(\n",
    "                targets, preds\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    def grad(self, inputs: tf.Tensor, targets: tf.Tensor) -> List:\n",
    "        with tf.GradientTape() as g:\n",
    "            error = self.loss(self(inputs), targets)\n",
    "        \n",
    "        return g.gradient(error, self.params)\n",
    "\n",
    "    def optimize(self, grads: List[tf.Tensor], rate: float) -> None:\n",
    "        opt = HyperParams.OPTIMIZER.value(learning_rate = rate)\n",
    "        opt.apply_gradients(zip(grads, self.params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data(xdat,ydat)\n",
    "data.partition()\n",
    "data.to_tensor()\n",
    "data.batch(HyperParams.BATCH_SIZE.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=385, shape=(105, 2), dtype=float32, numpy=\n",
       "array([[ 7.011201 , 10.428638 ],\n",
       "       [ 6.4876094,  9.6843405],\n",
       "       [ 6.8864193, 10.159925 ],\n",
       "       [ 7.027618 , 10.877469 ],\n",
       "       [ 6.1899724,  9.349236 ],\n",
       "       [ 5.899295 ,  9.236851 ],\n",
       "       [ 6.634941 , 10.303792 ],\n",
       "       [ 6.3556457,  9.915671 ],\n",
       "       [ 6.5774   , 10.100556 ],\n",
       "       [ 6.173556 ,  8.900406 ],\n",
       "       [ 8.312613 , 11.592333 ],\n",
       "       [ 7.7565374, 11.178372 ],\n",
       "       [ 6.9239273, 10.374707 ],\n",
       "       [ 7.5259185, 11.631507 ],\n",
       "       [ 6.8750277, 10.256214 ],\n",
       "       [ 6.7296205,  9.134917 ],\n",
       "       [ 6.400929 , 10.494539 ],\n",
       "       [ 6.0438766, 10.037334 ],\n",
       "       [ 6.5685263, 10.559129 ],\n",
       "       [ 6.467344 , 10.239203 ],\n",
       "       [ 7.0504007, 10.68489  ],\n",
       "       [ 6.1758404,  9.806005 ],\n",
       "       [ 6.3006225, 10.074718 ],\n",
       "       [ 7.143759 , 11.061439 ],\n",
       "       [ 7.2477202, 10.420727 ],\n",
       "       [ 7.7120805, 10.920318 ],\n",
       "       [ 7.0209007, 10.290879 ],\n",
       "       [ 7.687616 , 11.250873 ],\n",
       "       [ 6.8305697,  9.998159 ],\n",
       "       [ 6.0215592,  9.143281 ],\n",
       "       [ 6.8889365, 10.522208 ],\n",
       "       [ 6.8847275, 10.118455 ],\n",
       "       [ 5.94902  ,  9.676156 ],\n",
       "       [ 6.8583784, 10.3507   ],\n",
       "       [ 6.240564 ,  9.509198 ],\n",
       "       [ 4.460605 , 13.947837 ],\n",
       "       [ 4.742185 , 15.241556 ],\n",
       "       [ 3.7015915, 12.388789 ],\n",
       "       [ 4.4264407, 14.416151 ],\n",
       "       [ 3.559927 , 12.757879 ],\n",
       "       [ 4.040704 , 13.706301 ],\n",
       "       [ 3.8504772, 10.919502 ],\n",
       "       [ 4.762451 , 14.686694 ],\n",
       "       [ 3.362236 , 11.435149 ],\n",
       "       [ 3.712389 , 11.428371 ],\n",
       "       [ 3.85564  , 13.5948305],\n",
       "       [ 4.370453 , 12.124175 ],\n",
       "       [ 3.2789397, 12.328288 ],\n",
       "       [ 3.7734027, 14.265466 ],\n",
       "       [ 4.7879744, 14.133637 ],\n",
       "       [ 4.901365 , 14.498638 ],\n",
       "       [ 4.7352357, 15.198283 ],\n",
       "       [ 4.1591287, 14.795869 ],\n",
       "       [ 3.8330903, 13.244094 ],\n",
       "       [ 4.8242955, 12.620159 ],\n",
       "       [ 4.1023254, 12.362527 ],\n",
       "       [ 4.395521 , 12.837196 ],\n",
       "       [ 3.085596 , 13.539519 ],\n",
       "       [ 3.837523 , 12.925084 ],\n",
       "       [ 4.663786 , 14.729052 ],\n",
       "       [ 4.4308825, 14.2765875],\n",
       "       [ 3.8691788, 12.273932 ],\n",
       "       [ 3.7382736, 12.282758 ],\n",
       "       [ 3.428196 , 12.445892 ],\n",
       "       [ 3.9779043, 13.501261 ],\n",
       "       [ 4.2732573, 12.930767 ],\n",
       "       [ 3.9752584, 11.188215 ],\n",
       "       [ 3.8900375, 12.5831995],\n",
       "       [ 4.379559 , 11.122286 ],\n",
       "       [ 3.9756193, 12.595661 ],\n",
       "       [ 1.9042429, 13.748129 ],\n",
       "       [ 2.5375307, 12.946331 ],\n",
       "       [ 2.6051311, 14.419255 ],\n",
       "       [ 3.4353714, 17.170294 ],\n",
       "       [ 3.5612512, 16.616348 ],\n",
       "       [ 3.148765 , 15.331451 ],\n",
       "       [ 3.1434484, 15.570911 ],\n",
       "       [ 3.5438218, 14.137204 ],\n",
       "       [ 3.1884212, 14.321625 ],\n",
       "       [ 3.4335423, 14.998614 ],\n",
       "       [ 2.3743744, 12.742179 ],\n",
       "       [ 2.1195045, 12.62366  ],\n",
       "       [ 2.9310327, 13.840822 ],\n",
       "       [ 3.1850872, 13.817975 ],\n",
       "       [ 3.2309542, 15.081527 ],\n",
       "       [ 2.3901978, 12.32688  ],\n",
       "       [ 3.5251617, 17.586506 ],\n",
       "       [ 3.137597 , 14.704978 ],\n",
       "       [ 3.7849202, 16.11994  ],\n",
       "       [ 3.5274048, 13.688374 ],\n",
       "       [ 3.3170414, 13.407199 ],\n",
       "       [ 4.1306305, 16.252724 ],\n",
       "       [ 3.8066058, 16.750021 ],\n",
       "       [ 3.6205797, 14.187529 ],\n",
       "       [ 2.8653092, 14.118869 ],\n",
       "       [ 3.141213 , 14.244603 ],\n",
       "       [ 3.6989288, 15.120742 ],\n",
       "       [ 2.943017 , 14.608662 ],\n",
       "       [ 3.8361511, 14.891215 ],\n",
       "       [ 2.5375307, 12.946331 ],\n",
       "       [ 2.8799849, 14.9469385],\n",
       "       [ 2.7885032, 14.489253 ],\n",
       "       [ 3.4032168, 14.283791 ],\n",
       "       [ 2.5775466, 13.34395  ],\n",
       "       [ 2.82295  , 13.056913 ]], dtype=float32)>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = Dense(4,2, tf.nn.relu)\n",
    "layer(data.xtrn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Chain([\n",
    "    Dense(data.xtrn.shape[1], HyperParams.HIDDEN_NEURONS.value, HyperParams.ACTIVATION),\n",
    "    Dense(HyperParams.HIDDEN_NEURONS.value, HyperParams.OUTPUT_NEURONS.value, HyperParams.NORMALIZER)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y, yhat):\n",
    "    j = 0; correct = []\n",
    "    for i in tf.argmax(y, 1):\n",
    "        if i == tf.argmax(yhat[j]):\n",
    "            correct.append(1)\n",
    "        \n",
    "        j += 1\n",
    "    \n",
    "    num = tf.cast(tf.reduce_sum(correct), dtype = tf.float32)\n",
    "    den = tf.cast(y.shape[0], dtype = tf.float32)\n",
    "    return num / den"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    0 \t Training Error: 9.2442 \t Testing Error: 6.6833 \t Accuracy Training: 0.3333 \t Accuracy Testing: 0.5238\n",
      "Epoch:   20 \t Training Error: 6.3988 \t Testing Error: 4.7003 \t Accuracy Training: 0.3333 \t Accuracy Testing: 0.5238\n",
      "Epoch:   40 \t Training Error: 3.5070 \t Testing Error: 2.5939 \t Accuracy Training: 0.3333 \t Accuracy Testing: 0.5238\n",
      "Epoch:   60 \t Training Error: 1.7858 \t Testing Error: 1.3807 \t Accuracy Training: 0.4381 \t Accuracy Testing: 0.5238\n",
      "Epoch:   80 \t Training Error: 1.3339 \t Testing Error: 1.0511 \t Accuracy Training: 0.4571 \t Accuracy Testing: 0.5397\n",
      "Epoch:  100 \t Training Error: 0.9754 \t Testing Error: 0.7899 \t Accuracy Training: 0.4667 \t Accuracy Testing: 0.5238\n",
      "Epoch:  120 \t Training Error: 0.7031 \t Testing Error: 0.6033 \t Accuracy Training: 0.5429 \t Accuracy Testing: 0.5873\n",
      "Epoch:  140 \t Training Error: 0.5585 \t Testing Error: 0.5117 \t Accuracy Training: 0.7524 \t Accuracy Testing: 0.7778\n",
      "Epoch:  160 \t Training Error: 0.4834 \t Testing Error: 0.4747 \t Accuracy Training: 0.8095 \t Accuracy Testing: 0.7778\n",
      "Epoch:  180 \t Training Error: 0.4495 \t Testing Error: 0.4741 \t Accuracy Training: 0.8762 \t Accuracy Testing: 0.8254\n",
      "Epoch:  200 \t Training Error: 0.4424 \t Testing Error: 0.4952 \t Accuracy Training: 0.8667 \t Accuracy Testing: 0.8254\n",
      "Epoch:  220 \t Training Error: 0.4415 \t Testing Error: 0.5144 \t Accuracy Training: 0.8667 \t Accuracy Testing: 0.8571\n",
      "Epoch:  240 \t Training Error: 0.4391 \t Testing Error: 0.5164 \t Accuracy Training: 0.8667 \t Accuracy Testing: 0.8571\n",
      "Epoch:  260 \t Training Error: 0.4210 \t Testing Error: 0.4603 \t Accuracy Training: 0.8952 \t Accuracy Testing: 0.8095\n",
      "Epoch:  280 \t Training Error: 0.4276 \t Testing Error: 0.4424 \t Accuracy Training: 0.8190 \t Accuracy Testing: 0.7778\n",
      "Epoch:  300 \t Training Error: 0.4207 \t Testing Error: 0.4313 \t Accuracy Training: 0.8190 \t Accuracy Testing: 0.7778\n",
      "Epoch:  320 \t Training Error: 0.4092 \t Testing Error: 0.4222 \t Accuracy Training: 0.8190 \t Accuracy Testing: 0.7778\n",
      "Epoch:  340 \t Training Error: 0.3999 \t Testing Error: 0.4146 \t Accuracy Training: 0.8381 \t Accuracy Testing: 0.7778\n",
      "Epoch:  360 \t Training Error: 0.3917 \t Testing Error: 0.4081 \t Accuracy Training: 0.8381 \t Accuracy Testing: 0.7937\n",
      "Epoch:  380 \t Training Error: 0.3845 \t Testing Error: 0.4021 \t Accuracy Training: 0.8381 \t Accuracy Testing: 0.7937\n",
      "Epoch:  400 \t Training Error: 0.3769 \t Testing Error: 0.3974 \t Accuracy Training: 0.8381 \t Accuracy Testing: 0.7937\n",
      "Epoch:  420 \t Training Error: 0.3703 \t Testing Error: 0.3931 \t Accuracy Training: 0.8476 \t Accuracy Testing: 0.7937\n",
      "Epoch:  440 \t Training Error: 0.3652 \t Testing Error: 0.3898 \t Accuracy Training: 0.8571 \t Accuracy Testing: 0.7937\n",
      "Epoch:  460 \t Training Error: 0.3593 \t Testing Error: 0.3863 \t Accuracy Training: 0.8571 \t Accuracy Testing: 0.7937\n",
      "Epoch:  480 \t Training Error: 0.3539 \t Testing Error: 0.3833 \t Accuracy Training: 0.8571 \t Accuracy Testing: 0.8095\n"
     ]
    }
   ],
   "source": [
    "epoch_trn_loss = []\n",
    "epoch_tst_loss = []\n",
    "epoch_trn_accy = []\n",
    "epoch_tst_accy = []\n",
    "for j in range(HyperParams.EPOCHS.value):\n",
    "    trn_loss = []; trn_accy = []\n",
    "    for i in range(len(data.xtrn_batches)):\n",
    "        model.backward(data.xtrn_batches[i], data.ytrn_batches[i])\n",
    "        ypred = model(data.xtrn_batches[i])\n",
    "        trn_loss.append(model.loss(ypred, data.ytrn_batches[i]))\n",
    "        trn_accy.append(accuracy(data.ytrn_batches[i], ypred))\n",
    "\n",
    "    trn_err = tf.reduce_mean(trn_loss).numpy()\n",
    "    trn_acy = tf.reduce_mean(trn_accy).numpy()\n",
    "\n",
    "    tst_loss = []; tst_accy = []\n",
    "    for i in range(len(data.xtst_batches)):\n",
    "        ypred = model(data.xtst_batches[i])\n",
    "        tst_loss.append(model.loss(ypred, data.ytst_batches[i]))\n",
    "        tst_accy.append(accuracy(data.ytst_batches[i], ypred))\n",
    "    \n",
    "    tst_err = tf.reduce_mean(tst_loss).numpy()\n",
    "    tst_acy = tf.reduce_mean(tst_accy).numpy()\n",
    "    \n",
    "    epoch_trn_loss.append(trn_err)\n",
    "    epoch_tst_loss.append(tst_err)\n",
    "    epoch_trn_accy.append(trn_acy)\n",
    "    epoch_tst_accy.append(tst_acy)\n",
    "    \n",
    "    if j % 20 == 0:\n",
    "        print(\"Epoch: {0:4d} \\t Training Error: {1:.4f} \\t Testing Error: {2:.4f} \\t Accuracy Training: {3:.4f} \\t Accuracy Testing: {4:.4f}\".format(j, trn_err, tst_err, trn_acy, tst_acy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
